{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelForCausalLM, BartForConditionalGeneration, BartTokenizer, T5ForConditionalGeneration\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import Dataset\n",
    "from os import walk, path\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50262, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50262, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model = ''\n",
    "fields = []\n",
    "device = torch.device(\n",
    "    \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "generated_sequence = None\n",
    "MAX_LEN = 700\n",
    "model = None\n",
    "model_id = None\n",
    "# Load pre-trained model (weights)\n",
    "# model_name = 'EleutherAI/gpt-neo-125M'\n",
    "model_name = 'google/t5-v1_1-base'\n",
    "# model_name = 'facebook/bart-base'\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\n",
    "#     model_name,\n",
    "#     output_attentions=True,\n",
    "#     return_dict=True\n",
    "# )\n",
    "model_name = 'gpt2'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "# model = BartForConditionalGeneration.from_pretrained(\n",
    "#     model_name,\n",
    "#     output_attentions=True,\n",
    "#     return_dict=True\n",
    "# )\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, return_dict=True)\n",
    "tokenizer.add_special_tokens({\n",
    "    'pad_token': '<PAD>',\n",
    "    'bos_token': '<BOS>',\n",
    "    'eos_token': '<EOS>',\n",
    "    'sep_token': '<SEP>',\n",
    "    'additional_special_tokens': ['<SEPO>']\n",
    "})\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "checkpoint_name = 'checkpoint_gpt2_splitted_500_entities-epoch=02-val_loss=1.16.ckpt'\n",
    "checkpoint = torch.load('../checkpoints/' + checkpoint_name, map_location='cpu')\n",
    "state_dict = checkpoint['state_dict']\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    if k[:6] == 'model.':\n",
    "        name = k[6:]\n",
    "    else:\n",
    "        name = k\n",
    "    new_state_dict[name] = v\n",
    "model.load_state_dict(new_state_dict)\n",
    "# # torch.save(self.model_3.state_dict(), '/content/drive/MyDrive/Muteffstage/Checkpoints/only-mutation-epoch=09.ckpt')\n",
    "\n",
    "\n",
    "# # # model 2\n",
    "# # model_3.load_state_dict(\n",
    "# # torch.load('/content/drive/MyDrive/Tesi Polimi/GEO-metadata-translator-master/Checkpoints/checkpoint_2_hs_at_an-epoch=17-val_loss=0.156.ckpt')\n",
    "# # )\n",
    "\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference entities classic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:00<00:06,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> record date: 2106-02-12 campbell orthopedic associates 4 madera circle omak, ga 28172 habib valenzuela, m.d. valdez, harlan jr. 845-41-54-4 february 12, 2106 har is a 43 year old 6' 214 pound gentleman who is referred for consultation by dr. harlan oneil. about a week ago he slipped on the driveway at home and sustained an injury to his left ankle. he was seen at tri-city hospital and was told he had a fracture. he was placed in an air splint and advised to be partial weight bearing, and he is using a cane. he is here for routine follow-up. past medical history is notable for no ankle injuries previously. he has a history of diabetes and sleep apnea. he takes prozac, cardizem, glucophage and amaryl. he is also followed by dr. harold nutter for an arrhythmia. he does not smoke. he drinks minimally. he is a set designer at columbia pictures. on examination today he has slight tenderness of the left ankle about four fingerbreadths above the malleolus. the malleolus is non-tender medially or laterally with no ligamentous tenderness either. dorsal flexion and plantar flexion is without pain. there is no significant swelling. there are no some skin changes with some small abrasions proximally. there is no fibular tenderness proximally. no anterior pain is noted. no hindfoot, midfoot or forefoot tenderness is noted. i would like him to use a tube sock with his air cast. he is using a cane for ambulation. his x-rays do not show a notable fracture pattern today, and we will await the radiology opinion. i would like him to stay in the air splint with the sock. i will see him back in six weeks for review at the boxborough office. diagnosis: left ankle fracture. __________________________________________ habib valenzuela, m.d. hv/kuntz mmedical cc: harlan oneil, m.d. harold nutter, m.d. doctors hospital north 64 bruce st omak, ga 72196 habib valenzuela, m.d. dd: 02/12/<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS>06 dt: 02/17/06 dv: 02/12/06 ******** not reviewed by attending physician ********<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS> record date: 2108-03-14 campbell emergency dept visit valdez,harlan,jr. 845-41-54-4 visit date: 03/14/08 the patient was seen and examined in the emergency department. the patient was seen by the emergency medicine resident. i have discussed the management with the resident. i have also seen the patient primarily and reviewed the medical record. this is a brief addendum to the medical record. history of presenting complaint: briefly, this is a 45-year-old male who complains of several days of nausea, vomiting, and left lower quadrant discomfort. he also describes intermittent chest pain, which he has had for a number of months without significant change. he was sent in from his primary care doctor today with this pain and was also noted to have some ekg changes. the patient has no chest pain at the time of evaluation in the emergency department and no shortness of breath. review of systems: as indicated and otherwise negative. past medical history: as indicated in the chart. social history and family history: as indicated in the chart. physical examination: on physical examination, the patient is very well-appearing, a smiling, very pleasant gentleman in no acute distress. the blood pressure is 119/90, the pulse 82, and the temperature 97.9 degrees. normocephalic and atraumatic. the chest is clear to auscultation. the heart has a regular rate and rhythm. the abdomen is soft. he has left lower quadrant tenderness. he also, of note on cardiovascular examination, has a soft murmur which he says he has had since childhood. the extremities are normal. the neurologic examination is non-focal. therapy rendered/course in ed: this is a gentleman with abdominal pain who will receive a cat scan to rule out diverticulitis. he has also had some non-specific st changes on his ekg. he is pain-free at this time. he does not describe a classic exertional pattern for his chest pain, but given that he is a diabetic and with ekg changes, he will also be admitted for rule out mi. a ct is pending at the time of this dictation. disposition (including condition upon discharge): as above. the patient's condition is currently stable. ___________________________________ ck498/89095 jay carroll, m.d. jc72 d:03<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS>/14/08 t:03/14/08 dictated by: jay carroll, m.d. jc72 ******** not reviewed by attending physician ********<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS> record date: 2109-09-14 september 14, 2109 vicente blair, m.d. internal medical doctors hospital north omak, georgia 72196 re: valdez, harlan dhn#: 7672624 date of birth: 11/09/2062 current clinic visit date: 09/14/2109 dear vicente, thank you in advance for allowing me to share in the medical care of mr. harlan b. valdez, a 46-year-old male patient with prior polysomnographic evidence of sleep disordered breathing, as well as a history of difficulty in sleep, reinitiation and maintenance and increased early morning awakenings, as well as mixed systemic medical conditions. history of present illness: as you already know, mr. valdez who demonstrates a history of difficulties of sleep reinitiation and maintenance, as well as increased early morning awakenings, has noted an exacerbation of these sleep difficulties, occurring in temporal association with his loss of his wife from pancreatic cancer last year. he is now placed in the unfortunate situation of being a single parent to a 15-year-old son and a 10-year-old daughter and describes a modification of his current employment duties of a set designer. in particular, mr. valdez describes undergoing on frequent international travelling which has bee markedly curtailed as he is tending to his family situation closer to home. he described a history of intermittent snoring symptomatology but is unaware of specific nocturnal respiratory pauses. he is unaware of a \"restless\" lower limb sensory complaints which may impact on his ability to initiate or reinitiate sleep. he denies a history of a \" night owl\" personality or circadian rhythm dysfunction which may have played a role with respect to nocturnal sleep disruptions or sleep difficulties. he denies a history of paroxysmal abnormal disturbances or associated narcoleptic symptoms. mr. valdez underwent an initial formal polysomnographic evaluation at the center for sleep diagnostics at holy cross on 11/26/05, during which time he was noted to demonstrate a respiratory disturbance index of 81/hour, particularly exacerbated in the supine position and characterized predominantly by hypopneas, with equal distribution during non-rem and stage rem sleep and with associated o2 desaturation nadir of 88% the respiratory disturbances were predominantly obstructive or mixed hypop<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS>neas. in addition, loud snoring was noted. there was evidence of a sleep efficiency of 88% and a short sleep onset latency of 4 minutes. there was a predominance of \"light\" non-rem stages i-ii sleep, and a concomitant inability to achieve significant \"slow-wave\" or stage rem sleep. there was also \"alpha intrusions and alpha delta sleep\" evident during the initial sleep study. in addition premature ventricular contractions were noted. the patient underwent a cpap titration on 01/15/06, also at the tenacre foundation nursing home in boxborough, during which time there was a marked reduction in the frequency of hypopneas (respiratory disturbance index equals 2/hour) with cpap titrations between 4-6 cm. sleep efficiency improved to 91%, a short sleep onset latency was also noted (3 minutes). there was once again an increased predominance of \"light\" non-rem stage i-ii sleep, with concomitant inability to achieve sustained \"slow wave sleep\". since his initial trial of nocturnal cpap titration (at 6 cm of water pressure) and with various cpap mask modifications (including cpap nasal face mask and a mallinckrodt \"breeze\" supportive head gear with \"nasal pillows\". the patient describes associated claustrophobic symptomatology, relative difficulties with sustained nocturnal home cpap use, and difficulties with regards to cpap to being and complications by the bulkiness of the cpap machine in general. as a result, he has not utilized nocturnal cpap therapy for a period of time, although he still maintains the cpap equipment in his house. of particular note, and exacerbation of the past year, the patient demonstrates increased early morning awakenings (averaging 2-4 in number) with typical awakenings occurring approximately two hours after sleep initiation at 9:30 p.m. (the patient describes one awakening at 11:30 p.m. and the second awakening at 11:45 a.m., of unclear causative etiology). the patient then might awaken at 3 a.m. and be \"ready for the day\". if he is able to reinitiate sleep thereafter, the patient may demonstrate additional two early morning awakenings after a final awakening at 6 a.m. the patient is noted to have a history of mixed systemic conditions including diabetes, coronary artery disease,<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS> depressive disorder, as well as a relatively stable gastrointestinal condition, with no upper gi evidence of gastroparesis. medications: 1. provigil 200 mg p.o. q. a.m. prn. 2. lithium. 3. valproate. 4. glucophage 850 mg t.i.d. 5. humulin 15 units at night. 6. folate. 7. metoprolol. 8. cardia. 9. vitamin e. 10. coated aspirin. allergies/adverse reactions: the patient describes an enhancement to suicidal tendencies in association with prior prozac usage. social history: the patient denies active tobacco or alcoholic beverage usage. he has lost 15-20 pounds over the past several years. his current weight is 195 pounds. he is desirous of losing some additional weight with regards to more regular exercise, but his hectic social situation makes this somewhat difficult at the present time. on examination, the patient demonstrates a blood pressure of 146/88, (seated, left arm), respiratory rate 16. heent examination: borderline small posterior oropharyngeal aperture, with slightly increased redundant tissue evident posteriorly and a slightly elongated uvula noted. the patient appears awake, alert, with speech clear and fluent and receptive language function essentially intact. he is presently wearing dental braces. no obvious cranial nerve deficits are appreciated. no focal, sensory, motor or neurologic deficits are noted. no significant appendicular dystaxias or dysmetrias are currently in evidence. the routine gait appears to be normal based, without evidence of significant gait dystaxias. no current clinical ictal manifestations are present. no acute evidence of \"micro-sleeps\" are noted. impression: 1. sleep stage/arrousal dysfunction (780.56): manifested by subjective complaints of nonrestorative sleep, increased daytime fatigue and alternating hypersomnia, and recurrent polysomnographic evidence of \"lightened\" sleep pattern, with increased predominance of non-rem stages 1-2 sleep, and with the presence of \"alpha\" intrusions and \"alpha delta\" component to deeper sleep. these latter eeg findings have been described in association with subjective complaints of nonrestorative sleep, as well as clinical setting of chronic pain related complaints, depressive or anxiety disorder or intercurrent psychotropics agents used (but more usually associated with benzodiazepine or barbituate usage).<SEP>medications: <EOS>\n",
      "medications: <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:00<00:13,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> 2. sleep disordered breathing: as evidenced during prior polysomnographic evaluations, mostly of obstructive and or mixed hypopnea. the patient appears largely refractory to a trial of cpap therapy, particularly in so far as he demonstrates associated claustrophobic symptoms in association with it's usage, despite relatively modest cpap water pressures (6 cm). in addition, he has tried various nasal cpap face mask, including the mallinckrodt \"breeze\" supportive head gear with \"nasal pillows\" and with limited success. one might consider repeating a polysomnographic evaluation in the future, and if so, utilizing a potential trial of bipap titration, which may help to improve claustrophobic symptoms, but the patient will still be left with the issues referable to \"tangled tubing at night\" and issues referable to nasal face mask usage, as noted above. 3. relative difficulties in sleep reinitiation and maintenance: the patient describes at least 2-4 early morning awakenings with difficulty in sleep reinitiation and maintenance, thereby compounding his current sleep problem. while there would logically be a relationship between his current sleep exacerbations and the recent death of his wife from pancreatic cancer last year, there may also be evidence of other nocturnal sleep disturbances for which a repeat polysomnographic evaluation; i.e. in particular looking for the presence of increased spontaneous arousals or limb associated arousals or periodic limb movements of sleep may be of a special clinical benefit. plan: 1. in the short course, in so far as the patient describes himself as being exceedingly tired, and unable to perform the routine daily tasks of work and managing a family in the absence of his deceased wife, i have suggested initiation of prn zolpidem tartrate therapy, 5 mg tablets, utilizing one to two tablets p.o. q. h.s. prn for difficulties of sleep reinitiation and maintenance. 2. the patient is advised to take zolpidem tartrate therapy no more than 2-3 times per week, in an effort to avoid any issues of physiologic dependency. 3. the patient was advised against potential adverse behavioral and or systemic side effects of zolpidem tartrate therapy including hypersomnolence, gastric upset, loose stools, diarrhea, and or cardiac palpitations. pending his clinical response of his zolpidem tart<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS>rate therapy, i then might seek direct treatment for his sleep disordered breathing issues which may include a repeat sleep study with potential trial of bipap therapy (in an effort to modify or attenuate claustrophobic symptoms). if he proves poorly responsive to trial of bipap therapy however, i might consider supplemental o2 therapy at night and, with this in mind a follow up sleep study should have associated end-tidal co2 monitoring as well. 4. in the meantime, the patient was advised to contact the sleep disorders clinic for any acute sleep related concerns in the interim. 5. the patient may also benefit from nonpharmacologic approaches with regards to sleep reinitiation such as hypnotherapy, but i will hold off on these strategies pending follow up sleep disorders clinic evaluation (in approximately four months time). once again, thank you again for allowing me to share in the medical care of mr. harlan valdez. i hope this letter finds you well. sincerely yours, yovani vergara, m.d. sleep clinic doctors hospital north cc: sleep clinic dhn dd:09/14/2109 dt:09/15/2109 tx:24217 :1991<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS> record date: 2111-10-10 ccu jar transfer note admission date: 10/8/11 transfer: 10/10/11 patient name: valdez, harlan mrn#: 7672624 cardiologist: dr. nutter pcp: vicente barker cc: chest pain cath vf arrest rca stenting history of present illness (obtained on admission): pt is a 48 yo male with h/o dmii, hypercholesterolemia, bipolar d/o, and depression who began to have sub-sternal day prior to admission in car and pre-syncope + profound weakness. this cp was minimal, but the weakness made him pull over. he had a repeat of these symptoms day of admission. his ekg c/w 2/2107 showed flattened t-wave in v2 and twi in v3 and flattened t-waves in i, avl. his trop was negative, but mb index was elevated. due to t-wave flattening, history and elevated index it was decided to start on heparin and asa and take to cath lab. cath showed right dominant system with prox cx 40%, lad clear, rca prox 70-80% lesion and ostial pda 90%. during final dye injection, pt had vf arrest and 2 shocks. pt regained puls and was in af (new) with rvr. pt was started on amio. pt then began to experience discomfort in the rvr and it was decided to intervene. poba was done to ostial pda. a first no-eluting stent was placed in prox rca and pt had dissection and thus 2cd stent was placed. on admission to ccu, pt still in af with rvr (120's). he was on amio drip, bb, loaded on plavix, asa, lipitor, integrilin and was placed on avandia study. his complaint of some mild chest pain (not same as anginal pain day before) thought to be from defibrillation. past medical history: dmii, hyperchol, bipolar,htn, depression (s/p ect) medications on admission: asa, lipitor 20, lopressor 50 bid, folate, norvasc 5 qd; lithium, 300 bid; depakote 500 bid; sonata 10 mg qhs, doxylam<SEP>medications: <EOS>\n",
      "medications: <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:00<00:12,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>ine 25 qhs, mirtazapine 45 qd meds on transfer: please see green sheets medications: asa, lipitor 20, lopressor 50 bid, folate, norvasc 5 qd; lithium, 300 bid; depakote 500 bid; sonata 10 mg qhs, doxylamine 25 qhs, mirtazapine 45 qd allergies: nkda family history: family h/o cad social history: no etoh, no tob, no illicits review of systems: per hpi allergies: nkda family history: family h/o cad social history: no etoh, no tob, no illicits review of systems: per hpi ccu course + plan: 1) cards a. rhythm - on night of admission patient was started on an esmolol drip as well as amio bloused and rhythm converted to nsr. esmolol drip as well as amio was stopped and bb was escalated and patient has remained in nsr. i. ramp up lopressor as tolerated by bp b. pump - patient has remained euvolemic and had a echo with ef 84% and aortic stenosis c. ischemia - was stented x 2 to the prox rca lesion and was on integrilin x 24hrs prior. he was started on plavix. i. cont plavix, lopressor, lisinopril, lipitor, asa 2) psych - patient with long history of bipolar disorder + depression. he was on depakote, lithium and remeron as outpt. he was seen by psychiatry here. a. continue depakote and lithium (had subthereapeutic level that psych thought was likely due to non-compliance. b. continue remeron qhs c. f/u tsh 3) dm - blood sugars were originally elevated as amio drip he was originally on for af contained dextrose. he has remained on nph with riss. a. nph, riss 4) prophy - fragmin nexium labs + pe - see today's progress note ekg - afib with rvr, diffuse t-wave flattening impression: 48 yo male with h/o dmii, hypercholesterolemia, bipolar d/o, and depression and cad p/w cp and pre-sy<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS>cope found to have rca prox 70-80% lesion and ostial pda 90% (stents to rca and poba to pda). cath c/b vf arrest after dye load and resultant afib with rvr. plan: as outlined in ccu course. victor shepard md 39693<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS> record date: 2111-12-14 neurology cmf admission note name: valdez, harlan dob: 11-9-2062 mr#: 7672624 date: 12/13/11, 9:30pm for details, please see aaliyah iraheta's note id/cc: 49yorhm w/ pmh signif for cad, afib, dm, bipolar, who presents w/ rle weakness/decreased sensation x 24hrs hpi: 49yorhm w/ pmh signif for cad, afib, dm, bipolar, who was in usoh until 10d ago, when had weakness in his l arm. thought this was 2/2 sleeping on it. involved whole arm from shoulder down. was still doing his adl's but felt like the arm was slightly weaker, took 4-5 days to get back to normal. then, 12/12/11, was eating dinner w/ a friend, got up to go to br, found he had rle weakness/stiffness. noticed it was hard to climb stairs. went to sleep woke up at 2am nearly fell 2/2 weakness in his leg. in addition, has sensory loss in rle, esp foot - felt cold and dead. the sensory loss was worst distally, but extended up to his thigh. never had trouble speaking or understanding others, did not have a facial droop, no problems w/ his rue. stayed in bed until am, when he called his pcp, who told him to come to ed. of note, in oct-nov 2111, had cardiac cath x 2. had nstemi on oct 8, 2111 taken to cardiac cath cath showed significant rca disease and 40% prox lcx c/b vfib needing defib stent placed for stenotic rca lesion, c/b dissection needing 2nd stent placed. post cath had afib requiring amiodarone. then had subsequent cath nov 1 after an episode of cp, stents patent, other vessels stable. after his inpt cardiac hospitalization, was admitted to psych for 1 wk for suicidality/depression. previously, in spring 2110, had l eye visual disturbance nearly blind went to ghic where was dx'ed w/ retinal venous occlusion, treated w/ cort<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS>isone and laser surgery, w/ mild improvement in vision (now blurred in that eye). no ha, no tinnitus, no vertigo. no blurry vision except in l eye from venous occlusion, no diplopia. no problems w/ speech. pmh: dm - insulin x years cad s/p nstemi as above bipolar disorder - on lithium and depakote, has required inpt hospitalization and ect in past afib as above hyperchol meds: insulin 70/30 22 units bid asa 81 plavix 75 lithium? dose (600 bid on prior note) norvasc? dose (5 on prior note) mirtazapine? dose (45 on prior note) naltrexone nexium (20 on prior note) lipitor (40 on prior note) cozaar? dose depakote? dose (500 bid on prior note) (lopressor 75 po q8 per prior notes) folate all: lisinopril cough sh: tob: occas cigar etoh: 0 (used to have etoh problems) ivda: 0 lives w/ 17yo son. has 13yo daughter who lives w/ his sister. widower x 3yrs. fh: mom w/ pm at age 50, died of mi at 71. father w/ etoh, htn. sister w/ 4 miscarriages. vs: 129/80 89 20 97% on ra general: wnwd, nad heent: nc/at. no scleral icterus. mmm. op benign. neck: supple, no carotid bruits. cv: rrr s1, s2. ii-iii/vi sys murmur best at rusb rad to clav and neck.? second murmur at axilla systolic as well vs. galiverdin's sign. resp: ctab. no r/w/r abd: +bs. soft/nt/nd. ext: no c/c/e, dp 2+ bilat. skin: no rashes, intact. neuro: ms: conversationally intact. cn: ii,iii- pupils 5mm, round, reactive to light to 3mm; visual field full to confrontation; optic discs sharp iii,iv,vi-extraocular movements full, w/o nystagmus, l eyelid<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS> is slightly weaker longstanding since l eye problem v--sensation to lt and pin-prick intact bilaterally vii-facial expression muscles symmetric without weakness ix,x--palate elevates symmetrically xi--scms 5/5 xii--tongue protrudes midline motor: normal bulk and tone; no tremor. no pronator drift. delt elf ele wre fe io hpf kne knf dors plan ehl l 5 5 5 5 5 5 5 5 5 5 5 5 r 5 5 5 5 5 5 5 5 5 4+ 5 5 sensory: endorses a difference in sensation to lt and temp rle vs. lle. zone of difference (feels colder) is most pronounced r lateral calf area, not a particular dermatomal distribution, also w/ toe position sense mildly decreased r great toe. mild vibratory loss to ankles bilat symmetrical. reflex: bic tric bra hoff pat ank toes l 2 2 2 neg 2 2 down r 2+ 2 2+ neg 3 2 up? coord: no dysmetria on finger-nose-finger or heel-knee-shin gait: favors l leg slightly when walking romberg: normal labs/studies: chemistry lytes/renal/glucose sodium 136 135-145 mmol/l 12/13/11 18:35 133(l) 12/13/11 09:43 potassium 3.6 3.4-4.8 mmol/l 12/13/11 18:35 4.9(h) 10/31/11 12:55 chloride 109h 100-108 mmol/l 12/13/11 18:35 109(h) 12/13/11 18:35 carbon dioxide 27.0 23.0-31.9 mmol/l 12/13/11 18:35 22.9(l) 12/10/11 20:33 bun 11 8-25 mg/dl 12/13/11 18:35 creatinine 1.0 0.6-1.5 mg/dl 12/13/11 18:35 glucose 216h 70-110 mg/dl 12/13/11 18:35 216(h) 12/13/11 18:35 general chemistries calcium 9.4 8.5-10.5 mg/dl 12/13/11 10:40 8.3(l) 10<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS>/10/11 06:15 phosphorus 3.2 2.6-4.5 mg/dl 12/13/11 10:40 2.4(l) 11/02/11 15:10 magnesium 1.6 1.4-2.0 meq/l 12/13/11 10:40 lipid tests cholesterol 173 mg/dl 12/10/11 20:33 228(h) 09/03/07 10:46 triglycerides 547h 40-150 mg/dl 12/10/11 20:33 547(h) 12/10/11 20:33 hdl cholesterol 25l 35-100 mg/dl 12/10/11 20:33 25(l) 12/10/11 20:33 ldl cholesterol -- mg/dl 12/10/11 20:33 chol/hdl ratio 6.9 12/10/11 20:33 chemistry miscellaneous calc mean bld... 254 mg% 12/12/11 10:34 chemistry com... see detail 05/09/08 21:58 hemoglobin a1c 10.20h 3.80-6.40 % 12/12/11 10:34 10.20(h) 12/12/11 10:34 hematology complete blood count wbc 6.1 4.5-11.0 th/cmm 12/13/11 09:51 11.1(h) 10/08/11 20:16 rbc 4.51 4.50-5.90 mil/cm 12/13/11 09:51 4.37(l) 11/03/11 11:15 hgb 13.7 13.5-17.5 gm/dl 12/13/11 09:51 13.1(l) 11/01/11 04:52 hct 39.4l 41.0-53.0 % 12/13/11 09:51 39.4(l) 12/13/11 09:51 mcv 87 80-100 fl 12/13/11 09:51 mch 30.4 26.0-34.0 pg/rbc 12/13/11 09:51 mchc 34.9 31.0-37.0 g/dl 12/13/11 09:51 plt 210 150-350 th/cumm 12/13/11 09:51 rdw 13.<SEP>medications: <EOS>\n",
      "medications: <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:01<00:12,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>5 11.5-14.5 % 12/13/11 09:51 other hematology esr 14 0-17 mm/hr 12/13/11 19:51 urinalysis negative therapeutic drugs therapeutic drug monitoring lithium <0.10l 0.50-1.50 mmol/l 12/13/11 20:40 <0.10(l) 12/13/11 20:40 mri brain: acute/subacute infarcts (dwi bright/adc dark/flair bright) in l cerebellum (punctate) and r precentral gyrus (small elliptical area). ct-a head/neck: has aberrant origin of r vert from cca, both aca's come off of l carotid, also w/ bilateral fetal pca's and likely congenitally small vertebrobasilar vessels. no significant focal stenoses or atheromatous calcifications. ekg: pending mri l/s spine: negative impression: 49yorhm w/ pmh signif for cad, dm, bipolar disorder, afib, s/p recent cardiac cath who presents w/ lue weakness 10d prior to admission resolving after 4-5 days, and rle weakness?/sensory deficit? imaging reveals r precentral gyrus small infarct, l cerebellar infarct, no significant vessel stenoses. neuro exam w/ brisker reflexes on r, equivocal rle weakness, r sensory sx,? r upgoing toe. clinical picture and imaging are not consistent. unclear if pt is a poor historian and current sx are non-objective sensory findings and meaningful event was lue weakness 4-5d ago. neuro: stroke w/u including tte/holter, lipids/lipoprotein/homocysteine. --will also send hypercoag w/u (including hypercoag panel, pt20210, factor v leiden, apla, lupus anticoagulant) given hx of retinal venous thrombus and young age. --will also send bcx2 given recent cardiac cath, although esr wnl reassuring re? of endocarditis. --will check a1c re? of adequate dm control. --given psych hx, will check tox screen and lft's. --un<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS>clear if afib was only in context of post-cath, post vfib. will look for lae, holter abnl. could make a case to anti-coagulate regardless as has had documented afib. cv: hold anti-htn meds now. continue lipitor at outpt dose. allow sbp up to 180. psych: continue w/ depakote and lithium. mood is ok now, but will need to be monitored. fen: no ivf, ada 1800 low chol/low fat diet. endo: nph 20 bid for now, titrate up as needed, riss. checking a1c. ppx: put on sc fragmin, nexium. pneumoboots. anna v. wendy-bird, md hpc neuro resident #48600 case discussed w/ vern snow, senior resident.<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS> record date: 2097-07-19 name: booker, jennifer mrn: 7275714 office visit here for f/u. feeling better since using capsaicin yesterday, and notes relief! dr. prater decreased atenolol bid. problems hepatitis c : liver biopsy 2093 with chronic hepatitis, stage iii/iv, bridging fibrosis, failed interferon hypothyroidism hypertension bipolar disease smoking otalgia : possible neuralgia diabetes mellitus obsessive compulsive disorder medications synthroid (levothyroxine sodium) po qd (175mcg tablet take 1 tablet(s) x 90 days) tegretol (carbamazepine) po tid (100mg tablet chewable take 1 tablet(s)), 2 tablets qam, 3 tablets qpm wellbutrin (bupropion hcl) po qd (75mg tablet take 1 tablet(s)) ativan (lorazepam) po tid (0.5mg tablet take 1 tablet(s)) ativan (lorazepam) po qhs (1mg tablet take 1 tablet(s)) triazolam po qhs (0.25mg tablet take 0.5 -1 tablet(s)), must stop pm dose of ativan as per dr.good neurontin (gabapentin) po tid (600mg tablet take 1 tablet(s)), take 1 1/2 at bedtime xalatan (latanoprost) 1drop ou qam flonase (fluticasone nasal spray) 1-2spray nas qd humulin 70/30 (insulin 70/30 (human)) 20u qpm, 18 u qpm sc bid lisinopril po qd (5mg tablet take 1 tablet(s)) timolol maleate 0.5% 1drop os qpm albuterol inhaler 2puff inh qid prn prn oxycodone po 4-5 x per day prn pain in back, ribs (5mg tablet take 1 tablet(s)), had adverse reaction to duragesic, oxycontin makes nauseated. has returned patches to me - dr. tapia albuterol nebulizer 2.5mg neb q4-6h, premixed vials insulin regular humulin ( sliding scale sc, use as directed allergies aspirin - unknown sulfa - unknown codeine - unknown :<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS> tolerates oxycodone lmp 6/97. physical exam: well-appearing female, overweigh, coughing frequently. vital signs wt. declined p 80 bp 100/70 lungs clear, cor rrr s1s2 assessment: 1. arthritis: cpasacin, oxycodone 2. dm : fs 278, on lantus 46u, increase to 50 u qhs. 3. 4. hmc- needs td. rtc 8/97 _____________________________________________ mia e. tapia, m.d.<SEP>medications: <EOS>\n",
      "medications: <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [00:01<00:12,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> record date: 2084-12-27 jar night-float admission note internal medicine patient: mae paul mrn: 1005708 date of admission: 12/27/2084 renal attending: dr. john cc: cough, sob, failed z-pk x2 hpi: pt is a 73yo female with esrd on hs s/p kidney transplant x2. she has had a cough for over the past week. no fever; no sputum production. also reports 1 day of sore throat, now resolved and nasal congestion forh te past week. her grandson (age 4) has had a cough/head cold. her daughter is a surgeon and has prescribed her a z-pk x 2 which she has failed and yesterday gave her a dose of levaquin 500mg on 12/26/2084. last hd on 12/22; missed on 12/25. on presentation to the ed her vs were: t=97.9 p=118 bp=173/93 sat=91%ra. saturations decreased to 91%on 4l in setting of sbp to 221/119. she was treated with iv lopressor 2.5 x2 and lopressor 12.5 po. saturation improved to 95%4l. renal team evaluated her and she will have dialysis this am. she was admitted to floor for further evaluation. ros: as above. no chest pain.? wheezing today. + sob. no n/v/d. no rashes. no ha, muscle aches. + fatigue/generalized weakness. pmh: 1. esrd 2/2 gn on hd; status post left and right kidney transplants 2078 and 2075, which have failed. off immunosuppressive for the past 4 years. 2. htn 3. pvd: lower extremity pvd. this was shown on angio 9/83 (dr. xayasane), but she denies claudication. 4. r foot drop due to shingles 5. hemorrhoids: five- to six-year history constant anal protrusion and bloody discharge and mucus nearly constantly (wears a pad). no anal pain. no rectal surgeries. 6. status post supracervical hysterectomy (no bso) for benign disease. 4/82 underwent left salpingo-oophorectomy<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS> for serous cyst (dr. sutton). 7. status post appendectomy. 8. 8/81 she was treated for cmv colitis. 9. spinal stenosis and sciatica. current meds: daughter not present; need to verigy meds/doses pravachol norvasc phoslo nephrocaps neurontin allergies: vasotec tape fam hx: nc social history: retired mathematician at vassar. widow. lives alone. has daughter in the area (a surgeon). no smoking. physical examination: exam: vs: t=99.7 hr=101 bp=159/88 rr=22 sao2= 92%6lra general: heent: nc/at, perrl, anicteric. op w/ dmm, no erythema or injection. skin: no rashes neck: supple, 2+ carotids w/o bruits, no thyromegaly or nodules, no nodes chest: inspiratory crackles in right lower and mid lung fields, left side clear, no wheezing cvs: rrr nl s1 s2, no m/r/g abd: nl bs, soft, non-tender extr: no c/c/e 2+ dp's bilaterally neuro: aaox3, ms nl data: chemistries: 12/27/84 12/26/84 00:46 16:08 na 139 138 k 4.9(h) 5.6(h) cl 100 95(l) co2 24.4 21.8(l) bun 122(h) 110(h) cre 12.7(h) 11.7(h) glu 100 109 12/26/84 16:08 ca 10.1 phos 4.0 mg 2.1(h) tbili 0.3 dbili 0.1 tp 8.1 alb 3.7 glob 4.4(h) amy 114(h) lips 3.9 12/26/84 16:08 alt/sgpt 12 ast/sgot 21 alkp 221(h) tbili 0.3 dbili 0.1 12/26/84 16:08 chol 189(t) trig 224(h) hdl 65 heme: 12/26/84 15:<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS>52 wbc 8.1 hct 37.7 mcv 81 plt 326 method auto %neut 81(h) studies: cxr: pa and lateral chest 12/26/2084 comparison: 6/15/84. there are small bilateral pleural effusions. there is a new right lower the lobe opacity which appears to the heart border. this likely represents a right middle lobe pneumonia. there is atherosclerotic calcification of a tortuous aorta. the heart is enlarged. the mediastinal flow is unremarkable. there is stable mild wedge compression deformity and sclerosis of the thoracic spine. ekg: nsr, nl interval, nl axis, resolved twi isolated to iii assessment/plan: 73yo female with esrd presents with sb/ cough and rml pna on cxr. 1) pna: levo 250 qod; rml lobe on cxr. check sputum/blood cx. rapid flu neg; on precautions until final. 2) esrd: renal aware; hd in am 12/27; ca-phos product<50; cont phoslo. 3) cv: ischemia:? dynamic twi: cycle enzymes, asa/ lopressor/ pravachol pump: ef=66%; hd in am; lopressor/ norvac for bp 4) fen: s/p kayexcelate in ed; watch k 5) proph: nexium, heparin 6) code: full _____________________________ elinor keenan #10985<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS> record date: 2094-07-28 reason for visit comprehensive evaluation and follow-up of problems. problems shortness of breath 11/93 after starting atenolol (and wt gain) devel doe w/o wheezing. resolved off it (and w inc lasix 60 --> 80). ett 12/93 neg to 7 mets, echo 12/93 mild/mod mr, ao scle w/o sten, lv mild dil 55, la mod dil, ef 72%, no rwma, mod sym lvh, all c/w diastolic dysfunction. lumbar disc disease 4/94 seen for 9 days l buttock--> post thigh pain w/o trauma. no improvement at 2 weeks and mri done--> mild l4-5 and l5-s1 disc disease, facet djd and b foraminal encroachment l5-s1. sx resolved. spell 4/4/90 at gym while lying down with his head back, he had dizziness and then weakness. seen in cedars er with vital signs and sugar fine. ecg unchanged and not reproducible. turns out he's had similar sxs for about 30 years, always with his head back. tcds here okay except? of middle cerebral artery, resolved with spinal ct angio. no further episodes. hypertension present since mid 2060's. had been followed by dr. titus at dardanelle hospital. ett 5/12/82 to 98% prmhr showed st changes which were suggestive but not diagnostic and there was baseline lvh with strain but thallium was neg. there were occasional pvc's and couplets and two 3 beat runs of vt. had been acceptable albeit on an elaborate regimen that had evolved. suboptimal with additional weight gain 5/23/86. will recheck 6-8 weeks after 8-10 lb weight loss. ett thal scheduled, given abnl ecg, risk factors, and his continuing desire to exercise. 5/86 excellent exercise capacity to 9 mets, ecg nondiagnostic due to baseline abnormalities but thallium nl. 150/74 7/86 and will accept this for now but still needs to lose weight. 140/78 9/86 wo weight loss. rediscussed it would make it possible to simplify his regimen considerably. he remains on<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS> very elaborate regimen that he had been on for years by dr. titus. (8/8/87) [28-aug-2088] reasonable control on regimen, but still needs weight loss. 78-9 really not adequate control with his mild diabetes; increased cardura to 8 mg without help. change procardia xl to norvasc with increase from 15 to 20 mg. bp control acceptable. 7/91 still a bit suboptimal, and has continued so w wt gain. lisinopril increased from 40mg qd to bid, lasix from 40 to 60 qd. 7/92 still 140/70. wt down 5.5 from peak., at 228# not improved, so dec lisinopril to 40 mg and added irbesartan w good response, though 7/93 suboptimal 140/70 and subsequent ~ 135. inc 160-->240 7/94. following chems. continue to emphasize weight loss. hyperglycemia bs was apparently 190 fasting 2/82 but then lost 10 lbs. 3 hr pc bs is 91 3/3/82. rbs 118 and hba1c 5.06 = 83 4/22/83. 121 4/84. daughter did rbg 9/18/84 about 2 hr pp and was 208. 134 in office 9/22/84 and hba1c 8.16. 5.59 4/85. 2-1/2 hr pc 179 5/86. fbs 120 7/86. 12/86 fbs 145 and hgb a1c 4.57 = 66. fbs 154 and hba1c 6.44 = 129 on 4/87, done by hplc methods since there had been a discrepancy between blood sugars and hba1c levels, and this was in agreement with result by affinity. discussed dietary change at length. 8/87 visit. (8/8/87) a1c 6.10 = 117. blood sugars became higher, he did accept nutrition referral, begun on metformin 500 bid with gi sxs when 3rd dose added. 5/88 a1c 7.04 = 149. at that point had lost 14 pounds. [28-aug-2088] gets regular eye f/u. have emphasized importance of caring for feet. 7/94 beginning to get slight neuropathic pain. started on metform<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS>in with a1c normal at 500 bid. 1/90 a1c 6.60. urine microalbumin is high and this was discussed. already on good dose acei and try to control bp. gets regular eye exams. 4/91 a1c 6.00 on metformin 500mg tid. however, weight gained up to 234 1/92. a1c 3/92 7.40. metformin tolerated at 1000 bid. is aware of centrality of diet and wt loss. have discussed mulidisciplinary referral for wt loss, but he defers. 11/92 a1c 7.60 and glyburide added. 4/93 5.50 and dec to 1/2 of 2.5mg pill. 11/93 w creat metformin d/c. a1c off it 3/94 8.5 and glipizide inc to 10 mg, rec insulin but he deferred. walks playing golf but stopped other exercise; discussed. fbs and a1c done. gout has had various acute joint swellings, incl elbow and ankle. apparently dx of gout was made. he has noted this occurs especially if he eats lots of chocolate, which he tends to do frequently. last episode was abt a yr ago. no ho urolithiasis. discussed possible role of furosemide, and since it is not entirely clr to me why he needs to be on a high dose of furosemide it may be possible for this to be adjusted. however, no further episodes. until postop 3/86 attack. have reduced furosemide. spring 2088 having some mild sxs that could be podagra and some ankle sxs, though w/o overt inflammation. wonder whether left olecranon could be tophus. his right fourth toe also has a longstanding deformity which is not typical for tophus but could be. uric acid 7.8 1/88. uric acid done today. started on 0.6 colchicine daily. if uric acid high will start on low dose allopurinol in view of phospho-tophi. [28-aug-2088] may well be tophus on 4th toe, but uric acid level is normal. no attacks on colchicine. 7/94 dec to 1 qod. uric acid level done<SEP>medications: <EOS>\n",
      "medications: <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [00:02<00:13,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>. elevated serum cholesterol on atorva 10 mg ldl <100. fasting lipids done today. benign prostatic hypertrophy significant sx from bph, helped some by cardura, but retention and turp 3/86 by fonseca. residual prostate 1 to 2+ w/o nodules. currently on proscar w sx benefit. health maintenance screening flex sig done 7/84 neg by xian. quinlivan screening flex sig 10/30 neg x tics. stool cards given. skin lesion many actinic lesions. bce excised 12/89 at st francis. note that brother has had melanoma. dr. tomlin excised ssc from scalp 3/93-->reexcision. 7/94 will see him again.. allergies * nkda past medical history stopped smoking about 2061. no etoh. no known med allergies. family history mother died age 90. father died 37, pneumonia. two full brothers, one with dm, who is s/p thr, 1 a&w. two half brothers and one half sister, all a&w. one of his brothers had had melanoma. social history insurance claims examiner with desk job in new marlborough. married with 4 children. currently one of his sons is living at home. he is retired. goes to gym every day. plays alot of golf. wife with some memory lapses, which worries him because her mother had alzheimer's. review of systems l basal jt (thumb) discomfort and mild tenderness, and 10/91 discussed could be referred for injection if bothersome enough. otherwise negative except for extensive problem list. vital signs blood pressure 135/70 pulse 70 weight 231 lb physical exam skin many seborrheic keratoses, large hyperpigmented area on r shoulder. multiple viral warts under arms and on neck. actinic areas on scalp. heent oropharynx benign. tms clear. gets routine eye exams. neck no thyromegaly, increased jvp, bruits or adenopathy. chest clear. cor/cardiac ii/vi sem at apex without change. no gallop. abdomen obese without organomegaly or tenderness. no bruits. rectal exam prostate 2+ enlarged, smooth and without nodules. no change. genito-urinary normal. extremity trace edema left. pulses full. medications<SEP>medications: <EOS>\n",
      "medications: <EOS>\n",
      "<BOS> norvasc 10mg po bid enteric coated aspirin tab po qd potassium chloride 10meq po qam lisinopril 20mg po qd proscar (finasteride) 5mg 1 tablet(s) po qd lipitor (atorvastatin) 10mg po qd colchicine 0.6 mg po qod avapro (irbesartan) 150mg, 1.5 tablet(s) po qd allopurinol 100mg, 1 tablet(s) po qd furosemide 40 mg po 2 qd cardura 8 mg po qd glipizide 10mg, 1 tablet(s) po qd disposition and plans 3 months fasting or sooner prn.<SEP>medications: <EOS>\n",
      "medications: <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000003?line=59'>60</a>\u001b[0m input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000003?line=60'>61</a>\u001b[0m \u001b[39m# print('inputs ids length:', input_ids.shape[1])\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000003?line=62'>63</a>\u001b[0m results \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000003?line=63'>64</a>\u001b[0m   input_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000003?line=64'>65</a>\u001b[0m   max_length\u001b[39m=\u001b[39;49mmax_lenght \u001b[39m+\u001b[39;49m \u001b[39m250\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000003?line=65'>66</a>\u001b[0m   pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000003?line=66'>67</a>\u001b[0m   eos_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000003?line=67'>68</a>\u001b[0m   bos_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mbos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000003?line=68'>69</a>\u001b[0m   \u001b[39m# num_beanum_beams=2,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000003?line=69'>70</a>\u001b[0m   \u001b[39m# top_k=0,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000003?line=70'>71</a>\u001b[0m   \u001b[39m# top_p=0,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000003?line=71'>72</a>\u001b[0m   \u001b[39m# do_sample=False,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000003?line=72'>73</a>\u001b[0m   \u001b[39m# repetition_penalty=3.,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000003?line=73'>74</a>\u001b[0m   \u001b[39m# length_penalty=0.1,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000003?line=74'>75</a>\u001b[0m   \u001b[39m# early_stopping=True,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000003?line=75'>76</a>\u001b[0m )[\u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000003?line=76'>77</a>\u001b[0m \u001b[39mprint\u001b[39m(results)\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000003?line=77'>78</a>\u001b[0m results \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m<SEP>\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py:1173\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1167'>1168</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1168'>1169</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1, but is \u001b[39m\u001b[39m{\u001b[39;00mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m when doing greedy search.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1169'>1170</a>\u001b[0m         )\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1171'>1172</a>\u001b[0m     \u001b[39m# 10. run greedy search\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1172'>1173</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1173'>1174</a>\u001b[0m         input_ids,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1174'>1175</a>\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1175'>1176</a>\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1176'>1177</a>\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1177'>1178</a>\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1178'>1179</a>\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1179'>1180</a>\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1180'>1181</a>\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1181'>1182</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1182'>1183</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1184'>1185</a>\u001b[0m \u001b[39melif\u001b[39;00m is_sample_gen_mode:\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1185'>1186</a>\u001b[0m     \u001b[39m# 10. prepare logits warper\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1186'>1187</a>\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1187'>1188</a>\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k, top_p\u001b[39m=\u001b[39mtop_p, temperature\u001b[39m=\u001b[39mtemperature, num_beams\u001b[39m=\u001b[39mnum_beams\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1188'>1189</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py:1524\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1520'>1521</a>\u001b[0m     unfinished_sequences \u001b[39m=\u001b[39m unfinished_sequences\u001b[39m.\u001b[39mmul((next_tokens \u001b[39m!=\u001b[39m eos_token_id)\u001b[39m.\u001b[39mlong())\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1522'>1523</a>\u001b[0m \u001b[39m# stop when each sentence is finished, or if we exceed the maximum length\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m unfinished_sequences\u001b[39m.\u001b[39mmax() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m stopping_criteria(input_ids, scores):\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1524'>1525</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m synced_gpus:\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1525'>1526</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  filenames = next(walk('../data/trainingdata_v3/dev/'), (None, None, []))[2]\n",
    "  filenames_txt = [filename for filename in filenames if 'txt' in filename]\n",
    "  filenames_txt.sort()\n",
    "  for filename_txt in tqdm(filenames_txt):\n",
    "    error_log = ''\n",
    "    df_ann = pd.read_csv('../data/trainingdata_v3/dev/' + filename_txt[:-4] + '.ann', sep ='\\t', names=['entity-event-context', 'classification-type', 'value'])\n",
    "    df_entities = pd.DataFrame()\n",
    "    if len(df_ann.apply(lambda row: row['value'].lower() if row['entity-event-context'][0]== 'T' else '', axis=1)) > 0:\n",
    "      df_entities['entities'] = df_ann.apply(lambda row: row['value'].lower() if row['entity-event-context'][0]== 'T' else '', axis=1)\n",
    "      df_entities = df_entities[df_entities['entities'] != '']\n",
    "      entities_trg = df_entities.drop_duplicates(['entities']).loc[:, 'entities'].tolist()\n",
    "    else:\n",
    "      entities_trg = []\n",
    "    # print(entities_trg)\n",
    "    with open('../data/trainingdata_v3/dev/'+filename_txt) as text_file:\n",
    "      original_text = ''.join(text_file.readlines())\n",
    "    # print(original_text)\n",
    "    # replace space new lines and tabs with spaces the drop duplicate spaces\n",
    "    text = original_text.replace('\\t', ' ').replace('\\n', ' ')\n",
    "    text_filtered = ''\n",
    "    for char_index, char in enumerate(text):\n",
    "        if char_index < (len(text)-1):\n",
    "            if not(text[char_index + 1] == ' ' and char == ' '):\n",
    "                text_filtered += char\n",
    "    text_filtered = text_filtered.lower() # uncase the text\n",
    "    # print(text_filtered)\n",
    "    attribute = 'medications: '\n",
    "    # input_text = bos + text_filtered + sep + attribute\n",
    "    input_text_ids = tokenizer.encode(\n",
    "        text_filtered,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # If a text is longer than 900 tokens I create slices to divide it\n",
    "    text_slices = []\n",
    "    start_slice = 0\n",
    "    max_lenght = 500\n",
    "    for end_slice in range(max_lenght, input_text_ids.shape[1], max_lenght):\n",
    "      text_slices.append([start_slice, end_slice])\n",
    "      start_slice = end_slice\n",
    "    text_slices.append([start_slice, input_text_ids.shape[1]])\n",
    "    # print(text_slices)\n",
    "\n",
    "    # gpt2 small can only manage 1000 tokens, I divide the text in more parts and\n",
    "    # give them as different input and the aggregate the results\n",
    "    entity_list = []\n",
    "    for text_slice in text_slices:\n",
    "\n",
    "      input_ids = torch.cat(\n",
    "          (\n",
    "              torch.tensor([[tokenizer.bos_token_id]]),\n",
    "              input_text_ids[:, text_slice[0]:text_slice[1]],\n",
    "              torch.tensor([[tokenizer.sep_token_id]]),\n",
    "              tokenizer.encode(attribute, return_tensors='pt')\n",
    "          ),\n",
    "          dim=-1\n",
    "      )\n",
    "\n",
    "      input_ids = input_ids.to(device)\n",
    "      # print('inputs ids length:', input_ids.shape[1])\n",
    "\n",
    "      results = tokenizer.decode(model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_lenght + 250,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        # num_beanum_beams=2,\n",
    "        # top_k=0,\n",
    "        # top_p=0,\n",
    "        # do_sample=False,\n",
    "        # repetition_penalty=3.,\n",
    "        # length_penalty=0.1,\n",
    "        # early_stopping=True,\n",
    "      )[0])\n",
    "      # print(results)\n",
    "      results = results.split('<SEP>')[1]\n",
    "      # here I postprocessing the results droping duplicates and filtering empty results\n",
    "      print(results)\n",
    "      results = results.split('medications: ')[1]\n",
    "      \n",
    "      if '<s>' in results:\n",
    "        results = results.split('<s>')[1].split('<EOS>')[0]\n",
    "      if '<EOS>' in results:\n",
    "        results = results.split('<EOS>')[0]\n",
    "      else:\n",
    "        results = ','.join(results.split(',')[:-1])\n",
    "      # else:\n",
    "      #   results = results.split('<EOS>')[0]\n",
    "      # print(results)\n",
    "      results = results.split(',')\n",
    "      # print(results)\n",
    "      results = list(set(results))\n",
    "      results = [result.strip() for result in results if result.strip() != '']\n",
    "      # print('results without duplicates:', results)\n",
    "      entity_list += results\n",
    "\n",
    "    # for each entity founded with the model I search the positions in the text\n",
    "    ann_text = ''\n",
    "    results_with_pos =[]\n",
    "    index = 0\n",
    "\n",
    "    for entity in entity_list:   \n",
    "      for punctuation in [',', '.', ' ', '\\n', '\\t', ':', ';', '(', '[', '{']:\n",
    "        search_start = 0\n",
    "        # if entity == 'ativan':\n",
    "        #   print(f\"entity : {entity} with pucntuation: {punctuation}  was found with start point {original_text.lower().find(punctuation + entity, search_start)}\")\n",
    "        while original_text.lower().find(punctuation + entity, search_start) != -1:\n",
    "          start = original_text.lower().find(punctuation + entity, search_start) + 1\n",
    "          end = start + len(entity)\n",
    "          search_start = end\n",
    "          pos = [start, end]\n",
    "          results_with_pos.append([entity, pos])\n",
    "          ann_text += 'T'+ str(index) + '\\t' + 'Drug' + ' ' + str(start) + ' ' + str(end) + '\\t' + entity + '\\n'\n",
    "          index += 1\n",
    "\n",
    "\n",
    "    error_log += 'target entities: ' + ','.join(entities_trg) + '\\n'\n",
    "    error_log += 'predicted entities: ' + ','.join(list(set(list(zip(*results_with_pos))[0]))) + '\\n' if len(results_with_pos)>0 else 'predicted entities: \\n'\n",
    "    error_log += '\\n'\n",
    "    if len(results_with_pos) > 0:\n",
    "      for result in list(set(list(zip(*results_with_pos))[0])):\n",
    "        if result not in entities_trg:\n",
    "          error_log += filename_txt[:-4] + '\\t' +\\\n",
    "            'wrong prediction: ' + result + '\\n'\n",
    "    \n",
    "    for entity_trg in entities_trg:\n",
    "      if len(results_with_pos) > 0: \n",
    "        if entity_trg not in list(zip(*results_with_pos))[0]:\n",
    "          error_log += filename_txt[:-4] + '\\t' +\\\n",
    "            'missing prediction: ' + entity_trg +'\\n'\n",
    "    error_log += '********************original_text*******************' + '\\n'\n",
    "    error_log += original_text + '\\n'\n",
    "\n",
    "    for text_slice_index, text_slice in enumerate(text_slices):\n",
    "      # print(tokenizer.decode(input_text_ids[:, text_slice[0]:text_slice[1]][0]))\n",
    "      error_log += '*********** text_slice_' + str(text_slice_index) + ' ***********\\n' \n",
    "      error_log += tokenizer.decode(input_text_ids[:, text_slice[0]:text_slice[1]][0]) + '\\n'\n",
    "\n",
    "    if not path.exists('../data/trainingdata_v3/error_logs/'):\n",
    "      os.mkdir('../data/trainingdata_v3/error_logs/')\n",
    "\n",
    "    log_dir_path = '../data/trainingdata_v3/error_logs/'+ checkpoint_name + '/'\n",
    "    if not path.exists(log_dir_path):\n",
    "      os.mkdir(log_dir_path)\n",
    "    \n",
    "    with open(log_dir_path + filename_txt[:-4] + '_error_log' + '.txt' , 'w') as file:\n",
    "      file.write(error_log)\n",
    "\n",
    "    inference_dir_path = '../data/trainingdata_v3/'+'inference/'\n",
    "    # print(ann_text)\n",
    "    if not path.exists(inference_dir_path):\n",
    "      os.mkdir(inference_dir_path)\n",
    "    with open(inference_dir_path + filename_txt[:-4] + '.ann' , 'w') as ann_file:\n",
    "      ann_file.write(ann_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spitted with spacy\n",
    "\n",
    "with torch.no_grad():\n",
    "  filenames = next(walk('../data/trainingdata_v3/dev/'), (None, None, []))[2]\n",
    "  filenames_txt = [filename for filename in filenames if 'txt' in filename]\n",
    "  filenames_txt.sort()\n",
    "  for filename_txt in tqdm(filenames_txt):\n",
    "    error_log = ''\n",
    "    df_ann = pd.read_csv('../data/trainingdata_v3/dev/' + filename_txt[:-4] + '.ann', sep ='\\t', names=['entity-event-context', 'classification-type', 'value'])\n",
    "    df_entities = pd.DataFrame()\n",
    "    if len(df_ann.apply(lambda row: row['value'].lower() if row['entity-event-context'][0]== 'T' else '', axis=1)) > 0:\n",
    "      df_entities['entities'] = df_ann.apply(lambda row: row['value'].lower() if row['entity-event-context'][0]== 'T' else '', axis=1)\n",
    "      df_entities = df_entities[df_entities['entities'] != '']\n",
    "      entities_trg = df_entities.drop_duplicates(['entities']).loc[:, 'entities'].tolist()\n",
    "    else:\n",
    "      entities_trg = []\n",
    "    # print(entities_trg)\n",
    "    with open('../data/trainingdata_v3/dev/'+filename_txt) as text_file:\n",
    "      text = ''.join(text_file.readlines())\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    max_lenght = 750\n",
    "    text_slices = []\n",
    "    sent_index = 0\n",
    "    sents = [text_preprocess(sent.text) for sent in doc.sents]\n",
    "    text_slice = ''\n",
    "    for sent in sents:\n",
    "        if len(tokenizer.encode(text_slice + sent)) > max_lenght:\n",
    "            text_slices.append(text_slice)\n",
    "            text_slice = sent\n",
    "        else:\n",
    "            text_slice += sent\n",
    "    text_slices.append(text_slice)\n",
    "\n",
    "    # gpt2 small can only manage 1000 tokens, I divide the text in more parts and\n",
    "    # give them as different input and the aggregate the results\n",
    "    attribute = 'medications: '\n",
    "    entity_list = []\n",
    "    for text_slice in text_slices:\n",
    "\n",
    "      input_ids = torch.cat(\n",
    "          (\n",
    "              torch.tensor([[tokenizer.bos_token_id]]),\n",
    "              tokenizer.encode(text[:len(text_slice)], return_tensors='pt'),\n",
    "              torch.tensor([[tokenizer.sep_token_id]]),\n",
    "              tokenizer.encode(attribute, return_tensors='pt')\n",
    "          ),\n",
    "          dim=-1\n",
    "      )\n",
    "\n",
    "      input_ids = input_ids.to(device)\n",
    "      # print('inputs ids length:', input_ids.shape[1])\n",
    "\n",
    "      results = tokenizer.decode(model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_lenght + 250,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        # num_beams=2, \n",
    "        # early_stopping=True,\n",
    "        top_k=1\n",
    "      )[0])\n",
    "      print(results) \n",
    "      results = results.split('<SEP>')[1]\n",
    "      \n",
    "      # here I postprocessing the results droping duplicates and filtering empty results\n",
    "      results = results.split('medications: ')[1]\n",
    "      # print(results)\n",
    "      if '<s>' in results:\n",
    "        results = results.split('<s>')[1].split('<EOS>')[0]\n",
    "      if '<EOS>' in results:\n",
    "        results = results.split('<EOS>')[0]\n",
    "      else:\n",
    "        results = ','.join(results.split(',')[:-1])\n",
    "      # else:\n",
    "      #   results = results.split('<EOS>')[0]\n",
    "      # print(results)\n",
    "      results = results.split(',')\n",
    "      # print(results)\n",
    "      results = list(set(results))\n",
    "      results = [result.strip() for result in results if result.strip() != '']\n",
    "      # print('results without duplicates:', results)\n",
    "      entity_list += results\n",
    "\n",
    "    # for each entity founded with the model I search the positions in the text\n",
    "    ann_text = ''\n",
    "    results_with_pos =[]\n",
    "    index = 0\n",
    "\n",
    "    for entity in entity_list:   \n",
    "      for punctuation in [',', '.', ' ', '\\n', '\\t', ':', ';', '(', '[', '{']:\n",
    "        search_start = 0\n",
    "        # if entity == 'ativan':\n",
    "        #   print(f\"entity : {entity} with pucntuation: {punctuation}  was found with start point {original_text.lower().find(punctuation + entity, search_start)}\")\n",
    "        while original_text.lower().find(punctuation + entity, search_start) != -1:\n",
    "          start = original_text.lower().find(punctuation + entity, search_start) + 1\n",
    "          end = start + len(entity)\n",
    "          search_start = end\n",
    "          pos = [start, end]\n",
    "          results_with_pos.append([entity, pos])\n",
    "          ann_text += 'T'+ str(index) + '\\t' + 'Drug' + ' ' + str(start) + ' ' + str(end) + '\\t' + entity + '\\n'\n",
    "          index += 1\n",
    "\n",
    "\n",
    "    error_log += 'target entities: ' + ','.join(entities_trg) + '\\n'\n",
    "    error_log += 'predicted entities: ' + ','.join(list(set(list(zip(*results_with_pos))[0]))) + '\\n' if len(results_with_pos)>0 else 'predicted entities: \\n'\n",
    "    error_log += '\\n'\n",
    "    if len(results_with_pos) > 0:\n",
    "      for result in list(set(list(zip(*results_with_pos))[0])):\n",
    "        if result not in entities_trg:\n",
    "          error_log += filename_txt[:-4] + '\\t' +\\\n",
    "            'wrong prediction: ' + result + '\\n'\n",
    "    \n",
    "    for entity_trg in entities_trg:\n",
    "      if len(results_with_pos) > 0: \n",
    "        if entity_trg not in list(zip(*results_with_pos))[0]:\n",
    "          error_log += filename_txt[:-4] + '\\t' +\\\n",
    "            'missing prediction: ' + entity_trg +'\\n'\n",
    "    error_log += '********************original_text*******************' + '\\n'\n",
    "    error_log += text + '\\n'\n",
    "\n",
    "    for text_slice_index, text_slice in enumerate(text_slices):\n",
    "      # print(tokenizer.decode(input_text_ids[:, text_slice[0]:text_slice[1]][0]))\n",
    "      error_log += '*********** text_slice_' + str(text_slice_index) + ' ***********\\n' \n",
    "      error_log += text_slice + '\\n'\n",
    "\n",
    "    if not path.exists('../data/trainingdata_v3/error_logs/'):\n",
    "      os.mkdir('../data/trainingdata_v3/error_logs/')\n",
    "\n",
    "    log_dir_path = '../data/trainingdata_v3/error_logs/'+ checkpoint_name + '/'\n",
    "    if not path.exists(log_dir_path):\n",
    "      os.mkdir(log_dir_path)\n",
    "    \n",
    "    with open(log_dir_path + filename_txt[:-4] + '_error_log' + '.txt' , 'w') as file:\n",
    "      file.write(error_log)\n",
    "\n",
    "    inference_dir_path = '../data/trainingdata_v3/'+'inference/'\n",
    "    # print(ann_text)\n",
    "    if not path.exists(inference_dir_path):\n",
    "      os.mkdir(inference_dir_path)\n",
    "    with open(inference_dir_path + filename_txt[:-4] + '.ann' , 'w') as ann_file:\n",
    "      ann_file.write(ann_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference spacy entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitted with spacy\n",
    "def text_preprocess(text):\n",
    "    for iter in range(10):\n",
    "      text = text.replace('\\n\\n', '\\n').replace('\\n ', '\\n').replace(' \\n', '\\n').replace('\\t', ' ')\n",
    "    text_filtered = ''\n",
    "    for char_index, char in enumerate(text):\n",
    "        if char_index < (len(text)-1):\n",
    "            if not(text[char_index + 1] == ' ' and char == ' '):\n",
    "                text_filtered += char\n",
    "    return text_filtered.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context inference\n",
    "\n",
    "with torch.no_grad():\n",
    "  filenames = next(walk('../data/trainingdata_v3/dev/'), (None, None, []))[2]\n",
    "  filenames_txt = [filename for filename in filenames if 'txt' in filename]\n",
    "  filenames_ann = [filename for filename in filenames if 'ann' in filename]\n",
    "  filenames_txt.sort()\n",
    "  filenames_ann.sort()\n",
    "  df_dataset = pd.read_csv('../data/trainingdata_v3/datasets/dev_dataset_gpt2_disposition.tsv', sep ='\\t')\n",
    "  \n",
    "  for filename_txt, filename_ann in tqdm(zip(filenames_txt, filenames_ann)):\n",
    "    a_index = 0\n",
    "    # print(filename_txt)\n",
    "    \n",
    "    # print(original_text)\n",
    "    # df_ann = pd.read_csv('../data/trainingdata_v3/dev/' + filename_ann, sep='\\t', names=['entity-event-context', 'classification-type', 'value'])\n",
    "    if len(df_dataset.loc[df_dataset['filename'] == filename_ann[:-4]]) > 0:\n",
    "      df_ann = df_dataset.loc[df_dataset['filename'] == filename_ann[:-4]]\n",
    "    else:\n",
    "      df_ann = pd.DataFrame()\n",
    "    ann_text = ''\n",
    "    t_index = 0\n",
    "    for index, row in df_ann.iterrows():\n",
    "      text = row['text']\n",
    "\n",
    "      attribute = 'medication: ' + row['value'] + '<SEPO>' + 'disposition:'\n",
    "      # input_text = bos + text_filtered + sep + attribute\n",
    "      input_text_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors='pt',\n",
    "          truncation=True,\n",
    "          max_length=950\n",
    "      )\n",
    "\n",
    "      input_ids = torch.cat(\n",
    "          (\n",
    "              torch.tensor([[tokenizer.bos_token_id]]),\n",
    "              input_text_ids,\n",
    "              torch.tensor([[tokenizer.sep_token_id]]),\n",
    "              tokenizer.encode(attribute, return_tensors='pt')\n",
    "          ),\n",
    "          dim=-1\n",
    "      )\n",
    "\n",
    "      input_ids = input_ids.to(device)\n",
    "      # print('inputs ids length:', input_ids.shape[1])\n",
    "\n",
    "      results = tokenizer.decode(model.generate(\n",
    "        input_ids,\n",
    "        max_length=1020,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        # num_beams=2, \n",
    "        # early_stopping=True,\n",
    "        top_k=1\n",
    "      )[0])\n",
    "      # print(row['disposition-type'])\n",
    "      results = results.split('<SEP>')[1]\n",
    "      # print('model prediction:', results)\n",
    "      ctxs = ['action', 'actor', 'negation', 'temporality', 'certainty']\n",
    "      for ctx in ctxs:\n",
    "        assert ctx in results, f'ctx: {ctx} is not in results'\n",
    "      ann_text += 'T'+ str(t_index) + '\\t' + 'Drug' + ' ' + str(row['start']) + ' ' + str(row['end']) + '\\t' + str(row['value']) + '\\n'\n",
    "      ann_text += 'E'+ str(t_index) + '\\t' + 'Disposition' + ':' + 'T'+ str(t_index) +'\\n'\n",
    "      for ctx in ctxs:\n",
    "        prediction = results.split(ctx + ': ')[1].split('<SEPO>')[0].split('<EOS>')[0]\n",
    "        ann_text += 'A' + str(a_index) + '\\t' + ctx[0].upper() + ctx[1:] + ' ' + 'E'+ str(t_index) + ' ' + prediction +'\\n'\n",
    "        a_index += 1\n",
    "      \n",
    "      t_index += 1\n",
    "\n",
    "    inference_dir_path = '../data//trainingdata_v3/'+'inference-context/'\n",
    "    # print(ann_text)\n",
    "    if not path.exists(inference_dir_path):\n",
    "      os.mkdir(inference_dir_path)\n",
    "    with open(inference_dir_path + filename_txt[:-4] + '.ann' , 'w') as ann_file:\n",
    "      ann_file.write(ann_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disposition inference\n",
    "\n",
    "with torch.no_grad():\n",
    "  filenames = next(walk('../data/trainingdata_v3/dev/'), (None, None, []))[2]\n",
    "  filenames_txt = [filename for filename in filenames if 'txt' in filename]\n",
    "  filenames_ann = [filename for filename in filenames if 'ann' in filename]\n",
    "  filenames_txt.sort()\n",
    "  filenames_ann.sort()\n",
    "  df_dataset = pd.read_csv('../data/trainingdata_v3/datasets/dev_dataset_gpt2_disposition_only.tsv', sep ='\\t')\n",
    "  \n",
    "  for filename_txt, filename_ann in tqdm(zip(filenames_txt, filenames_ann)):\n",
    "    a_index = 0\n",
    "    # print(filename_txt)\n",
    "    \n",
    "    # print(original_text)\n",
    "    # df_ann = pd.read_csv('../data/trainingdata_v3/dev/' + filename_ann, sep='\\t', names=['entity-event-context', 'classification-type', 'value'])\n",
    "    if len(df_dataset.loc[df_dataset['filename'] == filename_ann[:-4]]) > 0:\n",
    "      df_ann = df_dataset.loc[df_dataset['filename'] == filename_ann[:-4]]\n",
    "    else:\n",
    "      df_ann = pd.DataFrame()\n",
    "    ann_text = ''\n",
    "    t_index = 0\n",
    "    for index, row in df_ann.iterrows():\n",
    "      text = row['text']\n",
    "\n",
    "      attribute = 'medication: ' + row['value'] + '<SEPO>' + 'disposition: '\n",
    "      # input_text = bos + text_filtered + sep + attribute\n",
    "      input_text_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors='pt',\n",
    "          truncation=True,\n",
    "          max_length=950\n",
    "      )\n",
    "      print(input_text_ids.shape)\n",
    "      input_ids = torch.cat(\n",
    "          (\n",
    "              torch.tensor([[tokenizer.bos_token_id]]),\n",
    "              input_text_ids,\n",
    "              torch.tensor([[tokenizer.sep_token_id]]),\n",
    "              tokenizer.encode(attribute, return_tensors='pt')\n",
    "          ),\n",
    "          dim=-1\n",
    "      )\n",
    "\n",
    "      input_ids = input_ids.to(device)\n",
    "      # print('inputs ids length:', input_ids.shape[1])\n",
    "\n",
    "      results = tokenizer.decode(model.generate(\n",
    "        input_ids,\n",
    "        max_length=1020,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        # num_beams=2, \n",
    "        # early_stopping=True,\n",
    "        top_k=1\n",
    "      )[0])\n",
    "      # print(results)\n",
    "      results = results.split('<SEP>')[1]\n",
    "      # print('model prediction:', results)\n",
    "      prediction = results.split('disposition' + ': ')[1].split('<SEPO>')[0].split('<EOS>')[0].strip()\n",
    "      # if prediction == 'Undetermined':\n",
    "        # print('undeterminated trovato')\n",
    "      ann_text += 'T'+ str(t_index) + '\\t' + 'Drug' + ' ' + str(row['start']) + ' ' + str(row['end']) + '\\t' + str(row['value']) + '\\n'\n",
    "      ann_text += 'E'+ str(t_index) + '\\t' + prediction + ':' + 'T'+ str(t_index) +'\\n'\n",
    "      \n",
    "      t_index += 1\n",
    "\n",
    "    inference_dir_path = '../data/trainingdata_v3/'+'inference-disposition/'\n",
    "    # print(ann_text)\n",
    "    if not path.exists(inference_dir_path):\n",
    "      os.mkdir(inference_dir_path)\n",
    "    with open(inference_dir_path + filename_txt[:-4] + '.ann' , 'w') as ann_file:\n",
    "      ann_file.write(ann_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [20:19<00:00, 24.38s/it]\n"
     ]
    }
   ],
   "source": [
    "#inference entities\n",
    "\n",
    "\n",
    "filenames = next(walk('../data/trainingdata_v3/dev'), (None, None, []))[2]\n",
    "filenames_txt = [filename for filename in filenames if 'txt' in filename]\n",
    "filenames_txt.sort()\n",
    "# print(filenames_txt)\n",
    "for filename_txt in tqdm(filenames_txt):\n",
    "  error_log = ''\n",
    "  df_ann = pd.read_csv('../data/trainingdata_v3/dev/' + filename_txt[:-4] + '.ann', sep ='\\t', names=['entity-event-context', 'classification-type', 'value'])\n",
    "  df_entities = pd.DataFrame()\n",
    "  if len(df_ann.apply(lambda row: row['value'].lower() if row['entity-event-context'][0]== 'T' else '', axis=1)) > 0:\n",
    "    df_entities['entities'] = df_ann.apply(lambda row: row['value'].lower() if row['entity-event-context'][0]== 'T' else '', axis=1)\n",
    "    df_entities = df_entities[df_entities['entities'] != '']\n",
    "    entities_trg = df_entities.drop_duplicates(['entities']).loc[:, 'entities'].tolist()\n",
    "  else:\n",
    "    entities_trg = []\n",
    "  with open('../data/trainingdata_v3/dev/'+filename_txt) as text_file:\n",
    "    original_text = ''.join(text_file.readlines())\n",
    "  # print(original_text)\n",
    "  # replace space new lines and tabs with spaces the drop duplicate spaces\n",
    "  text = original_text.replace('\\t', ' ').replace('\\n', ' ')\n",
    "  text_filtered = ''\n",
    "  for char_index, char in enumerate(text):\n",
    "      if char_index < (len(text)-1):\n",
    "          if not(text[char_index + 1] == ' ' and char == ' '):\n",
    "              text_filtered += char\n",
    "  text_filtered = text_filtered.lower() # uncase the text\n",
    "  # print(text_filtered)\n",
    "  attribute = 'medications: '\n",
    "  # input_text = bos + text_filtered + sep + attribute\n",
    "  input_text_ids = tokenizer.encode(\n",
    "      text_filtered,\n",
    "      return_tensors='pt',\n",
    "      truncation=True,\n",
    "      max_length=750,\n",
    "      add_special_tokens=True\n",
    "  )\n",
    "\n",
    "  # If a text is longer than 900 tokens I create slices to divide it\n",
    "  text_slices = []\n",
    "  start_slice = 0\n",
    "  max_lenght = 550\n",
    "  for end_slice in range(max_lenght, input_text_ids.shape[1], max_lenght):\n",
    "    text_slices.append(tokenizer.decode(input_text_ids[0, start_slice:end_slice], skip_special_tokens=True))\n",
    "    start_slice = end_slice\n",
    "  text_slices.append(tokenizer.decode(input_text_ids[0,start_slice:], skip_special_tokens=True))\n",
    "\n",
    "  # print(text_slices)\n",
    "  # break\n",
    "  # gpt2 small can only manage 1000 tokens, I divide the text in more parts and\n",
    "  # give them as different input and the aggregate the results\n",
    "  entity_list = []\n",
    "  for text_slice in text_slices:\n",
    "\n",
    "    # input_ids = torch.cat(\n",
    "    #     (\n",
    "    #         tokenizer(attribute + tokenizer.decode(input_text_ids[0, text_slice[0]:text_slice[1]]), return_tensors='pt',add_special_tokens=True,)['input_ids'],\n",
    "            \n",
    "    #     ),\n",
    "    #     dim=-1\n",
    "    # )\n",
    "    # print('inputs ids length:', input_ids.shape[1])\n",
    "\n",
    "    results = model.predict(\n",
    "      attribute + text_filtered,\n",
    "      top_k=0\n",
    "      beam=1,\n",
    "      num_beams=1\n",
    "    )[0]\n",
    "    # results = results.split('<SEP>')[1]\n",
    "    # here I postprocessing the results droping duplicates and filtering empty results\n",
    "    # print(results)\n",
    "    # print('dopo parsing:', results)\n",
    "    results = results.split(',')\n",
    "    # print(results)\n",
    "    results = list(set(results))\n",
    "    results = [result.strip() for result in results if result.strip() != '']\n",
    "    # print('results without duplicates:', results)\n",
    "    entity_list += results\n",
    "\n",
    "  ann_text = ''\n",
    "  results_with_pos =[]\n",
    "  index = 0\n",
    "\n",
    "  for entity in entity_list:   \n",
    "    for punctuation in [',', '.', ' ', '\\n', '\\t', ':', ';', '(', '[', '{']:\n",
    "      search_start = 0\n",
    "      # if entity == 'ativan':\n",
    "      #   print(f\"entity : {entity} with pucntuation: {punctuation}  was found with start point {original_text.lower().find(punctuation + entity, search_start)}\")\n",
    "      while original_text.lower().find(punctuation + entity, search_start) != -1:\n",
    "        start = original_text.lower().find(punctuation + entity, search_start) + 1\n",
    "        end = start + len(entity)\n",
    "        search_start = end\n",
    "        pos = [start, end]\n",
    "        results_with_pos.append([entity, pos])\n",
    "        ann_text += 'T'+ str(index) + '\\t' + 'Drug' + ' ' + str(start) + ' ' + str(end) + '\\t' + entity + '\\n'\n",
    "        index += 1\n",
    "\n",
    "  # print(results_with_pos)\n",
    "  # print(list(zip(*results_with_pos)))\n",
    "  error_log += 'target entities: ' + ','.join(entities_trg) + '\\n'\n",
    "  error_log += 'predicted entities: ' + ','.join(list(set(list(zip(*results_with_pos))[0]))) + '\\n' if len(results_with_pos)>0 else 'predicted entities: \\n'\n",
    "  error_log += '\\n'\n",
    "  if len(results_with_pos) > 0:\n",
    "    for result in list(set(list(zip(*results_with_pos))[0])):\n",
    "      if result not in entities_trg:\n",
    "        error_log += filename_txt[:-4] + '\\t' +\\\n",
    "          'wrong prediction: ' + result + '\\n'\n",
    "  \n",
    "  for entity_trg in entities_trg:\n",
    "    if len(results_with_pos) > 0: \n",
    "      if entity_trg not in list(zip(*results_with_pos))[0]:\n",
    "        error_log += filename_txt[:-4] + '\\t' +\\\n",
    "          'missing prediction: ' + entity_trg +'\\n'\n",
    "  error_log += '********************original_text*******************' + '\\n'\n",
    "  error_log += original_text + '\\n'\n",
    "\n",
    "  # for text_slice_index, text_slice in enumerate(text_slices):\n",
    "    # print(tokenizer.decode(input_text_ids[:, text_slice[0]:text_slice[1]][0]))\n",
    "    # error_log += '*********** text_slice_' + str(text_slice_index) + ' ***********\\n' \n",
    "    # error_log += tokenizer.decode(input_text_ids[:, text_slice[0]:text_slice[1]][0]) + '\\n'\n",
    "\n",
    "  if not path.exists('../data/trainingdata_v3/error_logs/'):\n",
    "    os.mkdir('../data/trainingdata_v3/error_logs/')\n",
    "\n",
    "  log_dir_path = '../data/trainingdata_v3/error_logs/'+ checkpoint_name + '/'\n",
    "  if not path.exists(log_dir_path):\n",
    "    os.mkdir(log_dir_path)\n",
    "  \n",
    "  with open(log_dir_path + filename_txt[:-4] + '_error_log' + '.txt' , 'w') as file:\n",
    "    file.write(error_log)\n",
    "\n",
    "  inference_dir_path = '../data/trainingdata_v3/'+'inference/'\n",
    "  # print(ann_text)\n",
    "  if not path.exists(inference_dir_path):\n",
    "    os.mkdir(inference_dir_path)\n",
    "  with open(inference_dir_path + filename_txt[:-4] + '.ann' , 'w') as ann_file:\n",
    "    ann_file.write(ann_text)\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference con simple t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simplet5 import SimpleT5\n",
    "from os import walk, path\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleT5()\n",
    "checkpoint_name = 'simplet5-epoch-6-train-loss-0.2724-val-loss-0.1477'\n",
    "model.load_model(\"t5\",\"../checkpoints/\"+checkpoint_name, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.17k/1.17k [00:00<00:00, 373kB/s]\n",
      "Downloading: 100%|██████████| 773k/773k [00:00<00:00, 1.09MB/s] \n",
      "Downloading: 100%|██████████| 1.32M/1.32M [00:00<00:00, 1.94MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('t5-large')\n",
    "# model = T5ForConditionalGeneration.from_pretrained('../checkpoints/simplet5-epoch-4-train-loss-0.5191-val-loss-0.342')\n",
    "# model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk, path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 3.82 GiB total capacity; 2.89 GiB already allocated; 16.25 MiB free; 2.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=50'>51</a>\u001b[0m entity_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=51'>52</a>\u001b[0m \u001b[39mfor\u001b[39;00m text_slice \u001b[39min\u001b[39;00m text_slices:\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=52'>53</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=53'>54</a>\u001b[0m   \u001b[39m# input_ids = torch.cat(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=59'>60</a>\u001b[0m   \u001b[39m# )\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=60'>61</a>\u001b[0m   \u001b[39m# print('inputs ids length:', input_ids.shape[1])\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=62'>63</a>\u001b[0m   results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=63'>64</a>\u001b[0m     attribute \u001b[39m+\u001b[39;49m text_slice,\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=64'>65</a>\u001b[0m     num_beams\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=65'>66</a>\u001b[0m     top_k\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=66'>67</a>\u001b[0m     top_p\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=67'>68</a>\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=68'>69</a>\u001b[0m     repetition_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=69'>70</a>\u001b[0m     length_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=70'>71</a>\u001b[0m     early_stopping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=71'>72</a>\u001b[0m   )[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=73'>74</a>\u001b[0m   \u001b[39m# print(results)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=74'>75</a>\u001b[0m   \u001b[39m# inp_ids = tokenizer.encode(\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=75'>76</a>\u001b[0m   \u001b[39m#   attribute + text_filtered,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=103'>104</a>\u001b[0m   \u001b[39m# print(results)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=104'>105</a>\u001b[0m   \u001b[39m# print('dopo parsing:', results)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/run/media/giuseppe/ext4-data/Projects/track1-n2c2/notebooks/inference-2.ipynb#ch0000020?line=105'>106</a>\u001b[0m   results \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py:464\u001b[0m, in \u001b[0;36mSimpleT5.predict\u001b[0;34m(self, source_text, max_length, num_return_sequences, num_beams, top_k, top_p, do_sample, repetition_penalty, length_penalty, early_stopping, skip_special_tokens, clean_up_tokenization_spaces)\u001b[0m\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=459'>460</a>\u001b[0m input_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mencode(\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=460'>461</a>\u001b[0m     source_text, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, add_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=461'>462</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=462'>463</a>\u001b[0m input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=463'>464</a>\u001b[0m generated_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=464'>465</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=465'>466</a>\u001b[0m     num_beams\u001b[39m=\u001b[39;49mnum_beams,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=466'>467</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=467'>468</a>\u001b[0m     repetition_penalty\u001b[39m=\u001b[39;49mrepetition_penalty,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=468'>469</a>\u001b[0m     length_penalty\u001b[39m=\u001b[39;49mlength_penalty,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=469'>470</a>\u001b[0m     early_stopping\u001b[39m=\u001b[39;49mearly_stopping,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=470'>471</a>\u001b[0m     top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=471'>472</a>\u001b[0m     top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=472'>473</a>\u001b[0m     num_return_sequences\u001b[39m=\u001b[39;49mnum_return_sequences,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=473'>474</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=474'>475</a>\u001b[0m preds \u001b[39m=\u001b[39m [\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=475'>476</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mdecode(\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=476'>477</a>\u001b[0m         g,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=480'>481</a>\u001b[0m     \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m generated_ids\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=481'>482</a>\u001b[0m ]\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/simplet5/simplet5.py?line=482'>483</a>\u001b[0m \u001b[39mreturn\u001b[39;00m preds\n",
      "File \u001b[0;32m~/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py:1088\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1080'>1081</a>\u001b[0m     model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_attention_mask_for_generation(\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1081'>1082</a>\u001b[0m         inputs_tensor, pad_token_id, eos_token_id\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1082'>1083</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1084'>1085</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1085'>1086</a>\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1086'>1087</a>\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1087'>1088</a>\u001b[0m     model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_encoder_decoder_kwargs_for_generation(\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1088'>1089</a>\u001b[0m         inputs_tensor, model_kwargs, model_input_name\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1089'>1090</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1091'>1092</a>\u001b[0m \u001b[39m# 4. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=1092'>1093</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m~/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py:507\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=504'>505</a>\u001b[0m encoder_kwargs[\u001b[39m\"\u001b[39m\u001b[39mreturn_dict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=505'>506</a>\u001b[0m encoder_kwargs[model_input_name] \u001b[39m=\u001b[39m inputs_tensor\n\u001b[0;32m--> <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=506'>507</a>\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m]: ModelOutput \u001b[39m=\u001b[39m encoder(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoder_kwargs)\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/generation_utils.py?line=508'>509</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m~/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1011\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=997'>998</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=998'>999</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=999'>1000</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1007'>1008</a>\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1008'>1009</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1009'>1010</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1010'>1011</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1011'>1012</a>\u001b[0m         hidden_states,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1012'>1013</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1013'>1014</a>\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1014'>1015</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1015'>1016</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1016'>1017</a>\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1017'>1018</a>\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1018'>1019</a>\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1019'>1020</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1020'>1021</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1021'>1022</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1022'>1023</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1024'>1025</a>\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1025'>1026</a>\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1026'>1027</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:646\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=642'>643</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=643'>644</a>\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=645'>646</a>\u001b[0m self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m0\u001b[39;49m](\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=646'>647</a>\u001b[0m     hidden_states,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=647'>648</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=648'>649</a>\u001b[0m     position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=649'>650</a>\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=650'>651</a>\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=651'>652</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=652'>653</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=653'>654</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=654'>655</a>\u001b[0m hidden_states, present_key_value_state \u001b[39m=\u001b[39m self_attention_outputs[:\u001b[39m2\u001b[39m]\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=655'>656</a>\u001b[0m attention_outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m2\u001b[39m:]  \u001b[39m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:553\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=541'>542</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=542'>543</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=543'>544</a>\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=549'>550</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=550'>551</a>\u001b[0m ):\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=551'>552</a>\u001b[0m     normed_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=552'>553</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mSelfAttention(\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=553'>554</a>\u001b[0m         normed_hidden_states,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=554'>555</a>\u001b[0m         mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=555'>556</a>\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=556'>557</a>\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=557'>558</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=558'>559</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=559'>560</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=560'>561</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=561'>562</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_output[\u001b[39m0\u001b[39m])\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=562'>563</a>\u001b[0m     outputs \u001b[39m=\u001b[39m (hidden_states,) \u001b[39m+\u001b[39m attention_output[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:490\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=484'>485</a>\u001b[0m value_states \u001b[39m=\u001b[39m project(\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=485'>486</a>\u001b[0m     hidden_states, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv, key_value_states, past_key_value[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=486'>487</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=488'>489</a>\u001b[0m \u001b[39m# compute scores\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=489'>490</a>\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=490'>491</a>\u001b[0m     query_states, key_states\u001b[39m.\u001b[39;49mtranspose(\u001b[39m3\u001b[39;49m, \u001b[39m2\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=491'>492</a>\u001b[0m )  \u001b[39m# equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=493'>494</a>\u001b[0m \u001b[39mif\u001b[39;00m position_bias \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/giuseppe/.anaconda3/envs/data-science/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=494'>495</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_relative_attention_bias:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 3.82 GiB total capacity; 2.89 GiB already allocated; 16.25 MiB free; 2.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#inference entities\n",
    "\n",
    "\n",
    "filenames = next(walk('../data/trainingdata_v3/dev'), (None, None, []))[2]\n",
    "filenames_txt = [filename for filename in filenames if 'txt' in filename]\n",
    "filenames_txt.sort()\n",
    "# print(filenames_txt)\n",
    "for filename_txt in tqdm(filenames_txt):\n",
    "  error_log = ''\n",
    "  df_ann = pd.read_csv('../data/trainingdata_v3/dev/' + filename_txt[:-4] + '.ann', sep ='\\t', names=['entity-event-context', 'classification-type', 'value'])\n",
    "  df_entities = pd.DataFrame()\n",
    "  if len(df_ann.apply(lambda row: row['value'].lower() if row['entity-event-context'][0]== 'T' else '', axis=1)) > 0:\n",
    "    df_entities['entities'] = df_ann.apply(lambda row: row['value'].lower() if row['entity-event-context'][0]== 'T' else '', axis=1)\n",
    "    df_entities = df_entities[df_entities['entities'] != '']\n",
    "    entities_trg = df_entities.drop_duplicates(['entities']).loc[:, 'entities'].tolist()\n",
    "  else:\n",
    "    entities_trg = []\n",
    "  with open('../data/trainingdata_v3/dev/'+filename_txt) as text_file:\n",
    "    original_text = ''.join(text_file.readlines())\n",
    "  # print(original_text)\n",
    "  # replace space new lines and tabs with spaces the drop duplicate spaces\n",
    "  text = original_text.replace('\\t', ' ').replace('\\n', ' ')\n",
    "  text_filtered = ''\n",
    "  for char_index, char in enumerate(text):\n",
    "      if char_index < (len(text)-1):\n",
    "          if not(text[char_index + 1] == ' ' and char == ' '):\n",
    "              text_filtered += char\n",
    "  text_filtered = text_filtered.lower() # uncase the text\n",
    "  # print(text_filtered)\n",
    "  attribute = 'medications: '\n",
    "  # input_text = bos + text_filtered + sep + attribute\n",
    "  input_text_ids = tokenizer.encode(\n",
    "      text_filtered,\n",
    "      return_tensors='pt',\n",
    "      add_special_tokens=True\n",
    "  )\n",
    "\n",
    "  # If a text is longer than 900 tokens I create slices to divide it\n",
    "  text_slices = []\n",
    "  start_slice = 0\n",
    "  max_lenght = 250\n",
    "  for end_slice in range(max_lenght, input_text_ids.shape[1], max_lenght):\n",
    "    text_slices.append(tokenizer.decode(input_text_ids[0, start_slice:end_slice], skip_special_tokens=True))\n",
    "    start_slice = end_slice\n",
    "  text_slices.append(tokenizer.decode(input_text_ids[0,start_slice:], skip_special_tokens=True))\n",
    "\n",
    "  # print(text_slices)\n",
    "  # break\n",
    "  # gpt2 small can only manage 1000 tokens, I divide the text in more parts and\n",
    "  # give them as different input and the aggregate the results\n",
    "  entity_list = []\n",
    "  for text_slice in text_slices:\n",
    "\n",
    "    # input_ids = torch.cat(\n",
    "    #     (\n",
    "    #         tokenizer(attribute + tokenizer.decode(input_text_ids[0, text_slice[0]:text_slice[1]]), return_tensors='pt',add_special_tokens=True,)['input_ids'],\n",
    "            \n",
    "    #     ),\n",
    "    #     dim=-1\n",
    "    # )\n",
    "    # print('inputs ids length:', input_ids.shape[1])\n",
    "\n",
    "    results = model.predict(\n",
    "      attribute + text_slice,\n",
    "      num_beams=2,\n",
    "      top_k=0,\n",
    "      top_p=1,\n",
    "      do_sample=False,\n",
    "      repetition_penalty=0.5,\n",
    "      length_penalty=0.4,\n",
    "      early_stopping=True,\n",
    "    )[0]\n",
    "\n",
    "    # print(results)\n",
    "    # inp_ids = tokenizer.encode(\n",
    "    #   attribute + text_filtered,\n",
    "    #   return_tensors=\"pt\",\n",
    "    #   add_special_tokens=True,\n",
    "      \n",
    "    # )\n",
    "    # inp_ids = inp_ids.to(device)\n",
    "\n",
    "    # results_ids = model.generate(\n",
    "    #   inp_ids,\n",
    "    #   max_length=250,\n",
    "      # num_beams=2,\n",
    "      # top_k=50,\n",
    "      # top_p=0.95,\n",
    "      # do_sample=True,\n",
    "      # repetition_penalty=2.5,\n",
    "      # length_penalty=1.0,\n",
    "      # early_stopping=True,\n",
    "    # )\n",
    "\n",
    "    # results = tokenizer.decode(\n",
    "    #   results_ids[0],\n",
    "    #   skip_special_tokens=True,\n",
    "    #   clean_up_tokenization_spaces=True,\n",
    "    # )\n",
    "\n",
    "\n",
    "    # results = results.split('<SEP>')[1]\n",
    "    # here I postprocessing the results droping duplicates and filtering empty results\n",
    "    # print(results)\n",
    "    # print('dopo parsing:', results)\n",
    "    results = results.split(',')\n",
    "    # print(results)\n",
    "    results = list(set(results))\n",
    "    results = [result.strip() for result in results if result.strip() != '']\n",
    "    print('results without duplicates:', results)\n",
    "    entity_list += results\n",
    "\n",
    "  ann_text = ''\n",
    "  results_with_pos =[]\n",
    "  index = 0\n",
    "\n",
    "  for entity in entity_list:   \n",
    "    for punctuation in [',', '.', ' ', '\\n', '\\t', ':', ';', '(', '[', '{']:\n",
    "      search_start = 0\n",
    "      # if entity == 'ativan':\n",
    "      #   print(f\"entity : {entity} with pucntuation: {punctuation}  was found with start point {original_text.lower().find(punctuation + entity, search_start)}\")\n",
    "      while original_text.lower().find(punctuation + entity, search_start) != -1:\n",
    "        start = original_text.lower().find(punctuation + entity, search_start) + 1\n",
    "        end = start + len(entity)\n",
    "        search_start = end\n",
    "        pos = [start, end]\n",
    "        results_with_pos.append([entity, pos])\n",
    "        ann_text += 'T'+ str(index) + '\\t' + 'Drug' + ' ' + str(start) + ' ' + str(end) + '\\t' + entity + '\\n'\n",
    "        index += 1\n",
    "\n",
    "  # print(results_with_pos)\n",
    "  # print(list(zip(*results_with_pos)))\n",
    "  error_log += 'target entities: ' + ','.join(entities_trg) + '\\n'\n",
    "  error_log += 'predicted entities: ' + ','.join(list(set(list(zip(*results_with_pos))[0]))) + '\\n' if len(results_with_pos)>0 else 'predicted entities: \\n'\n",
    "  error_log += '\\n'\n",
    "  if len(results_with_pos) > 0:\n",
    "    for result in list(set(list(zip(*results_with_pos))[0])):\n",
    "      if result not in entities_trg:\n",
    "        error_log += filename_txt[:-4] + '\\t' +\\\n",
    "          'wrong prediction: ' + result + '\\n'\n",
    "  \n",
    "  for entity_trg in entities_trg:\n",
    "    if len(results_with_pos) > 0: \n",
    "      if entity_trg not in list(zip(*results_with_pos))[0]:\n",
    "        error_log += filename_txt[:-4] + '\\t' +\\\n",
    "          'missing prediction: ' + entity_trg +'\\n'\n",
    "  error_log += '********************original_text*******************' + '\\n'\n",
    "  error_log += original_text + '\\n'\n",
    "\n",
    "  # for text_slice_index, text_slice in enumerate(text_slices):\n",
    "    # print(tokenizer.decode(input_text_ids[:, text_slice[0]:text_slice[1]][0]))\n",
    "    # error_log += '*********** text_slice_' + str(text_slice_index) + ' ***********\\n' \n",
    "    # error_log += tokenizer.decode(input_text_ids[:, text_slice[0]:text_slice[1]][0]) + '\\n'\n",
    "\n",
    "  if not path.exists('../data/trainingdata_v3/error_logs/'):\n",
    "    os.mkdir('../data/trainingdata_v3/error_logs/')\n",
    "\n",
    "  log_dir_path = '../data/trainingdata_v3/error_logs/'+ checkpoint_name + '/'\n",
    "  if not path.exists(log_dir_path):\n",
    "    os.mkdir(log_dir_path)\n",
    "  \n",
    "  with open(log_dir_path + filename_txt[:-4] + '_error_log' + '.txt' , 'w') as file:\n",
    "    file.write(error_log)\n",
    "\n",
    "  inference_dir_path = '../data/trainingdata_v3/'+'inference/'\n",
    "  # print(ann_text)\n",
    "  if not path.exists(inference_dir_path):\n",
    "    os.mkdir(inference_dir_path)\n",
    "  with open(inference_dir_path + filename_txt[:-4] + '.ann' , 'w') as ann_file:\n",
    "    ann_file.write(ann_text)\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b4ae32079e0bf426878f2104226b4c202dc5dd07bf66a8fad266fc6f7ab6329"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('data-science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
